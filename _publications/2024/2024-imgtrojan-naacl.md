---
title:          "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image"
date:           2024-03-05 00:01:00 +0800
selected:       true
pub:            "North American Chapter of the Association for Computational Linguistics (NAACL)"
pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Oral</span>'
pub_date:       "2025"
abstract: >-
  There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. We propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. We demonstrate the efficacy of our attack by comparing it with baseline methods.
cover:          /assets/images/covers/imgtrojan_cover.png
authors:
  - Xijia Tao*
  - Shuai Zhong*
  - Lei Li*
  - Qi Liu
  - Lingpeng Kong
links:
  Paper: https://arxiv.org/abs/2403.02910
  ACL Anthology: https://aclanthology.org/2025.naacl-long.360/
  Code: https://github.com/xijia-tao/ImgTrojan
---
